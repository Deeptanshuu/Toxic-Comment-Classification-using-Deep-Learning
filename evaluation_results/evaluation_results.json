{
  "overall": {
    "loss": 0.18776385083473274,
    "auc": 0.9259171799699759,
    "auc_weighted": 0.9442696333538418,
    "precision": 0.5344061505907316,
    "recall": 0.7817561020978864,
    "f1": 0.6213496753848654
  },
  "per_language": {
    "0": {
      "auc": 0.9571257247394875,
      "auc_ci": [
        0.9535846877176878,
        0.9608150825426435
      ],
      "precision": 0.5555348282579734,
      "recall": 0.8468709800580542,
      "f1": 0.6444934767660943,
      "f1_ci": [
        0.6346287980073152,
        0.6741441905882308
      ],
      "sample_count": 4588,
      "class_metrics": {
        "toxic": {
          "auc": 0.9621138334064969,
          "precision": 0.842741935483871,
          "recall": 0.9604779411764706,
          "f1": 0.897766323024055,
          "specificity": 0.8383084577114428,
          "npv": 0.959203036053131,
          "threshold": 0.2747279405593872,
          "positive_samples": 2176,
          "true_positives": 2090,
          "false_positives": 390,
          "true_negatives": 2022,
          "false_negatives": 86
        },
        "severe_toxic": {
          "auc": 0.9499761279127704,
          "precision": 0.2720125786163522,
          "recall": 0.8781725888324873,
          "f1": 0.41536614645858344,
          "specificity": 0.8945570485083124,
          "npv": 0.9939271255060729,
          "threshold": 0.08588284254074097,
          "positive_samples": 197,
          "true_positives": 173,
          "false_positives": 463,
          "true_negatives": 3928,
          "false_negatives": 24
        },
        "obscene": {
          "auc": 0.957280595835101,
          "precision": 0.7737330478229836,
          "recall": 0.8791565287915653,
          "f1": 0.8230827638572513,
          "specificity": 0.9055141579731744,
          "npv": 0.9532475682459993,
          "threshold": 0.37424641847610474,
          "positive_samples": 1233,
          "true_positives": 1084,
          "false_positives": 317,
          "true_negatives": 3038,
          "false_negatives": 149
        },
        "threat": {
          "auc": 0.9697358146798543,
          "precision": 0.3629032258064516,
          "recall": 0.6617647058823529,
          "f1": 0.46875,
          "specificity": 0.9825221238938053,
          "npv": 0.994847670250896,
          "threshold": 0.11552593857049942,
          "positive_samples": 68,
          "true_positives": 45,
          "false_positives": 79,
          "true_negatives": 4441,
          "false_negatives": 23
        },
        "insult": {
          "auc": 0.9350142915734952,
          "precision": 0.6284848484848485,
          "recall": 0.9072615923009624,
          "f1": 0.7425707124955245,
          "specificity": 0.8220609579100145,
          "npv": 0.9639210347174949,
          "threshold": 0.26808470487594604,
          "positive_samples": 1143,
          "true_positives": 1037,
          "false_positives": 613,
          "true_negatives": 2832,
          "false_negatives": 106
        },
        "identity_hate": {
          "auc": 0.9686336850292083,
          "precision": 0.4533333333333333,
          "recall": 0.794392523364486,
          "f1": 0.5772495755517827,
          "specificity": 0.9531321444901691,
          "npv": 0.9895561357702349,
          "threshold": 0.1609325259923935,
          "positive_samples": 214,
          "true_positives": 170,
          "false_positives": 205,
          "true_negatives": 4169,
          "false_negatives": 44
        }
      }
    },
    "1": {
      "auc": 0.9256529605043847,
      "auc_ci": [
        0.9182179112919449,
        0.9335093340709781
      ],
      "precision": 0.5286366263533598,
      "recall": 0.7887276820073078,
      "f1": 0.6253670521397233,
      "f1_ci": [
        0.6057967259404745,
        0.6388693828157405
      ],
      "sample_count": 5193,
      "class_metrics": {
        "toxic": {
          "auc": 0.9700660212377463,
          "precision": 0.8730727859447831,
          "recall": 0.9401544401544402,
          "f1": 0.9053727458635434,
          "specificity": 0.8640030733768729,
          "npv": 0.9355241264559068,
          "threshold": 0.2784601151943207,
          "positive_samples": 2590,
          "true_positives": 2435,
          "false_positives": 354,
          "true_negatives": 2249,
          "false_negatives": 155
        },
        "severe_toxic": {
          "auc": 0.9032119421376732,
          "precision": 0.28324697754749567,
          "recall": 0.6721311475409836,
          "f1": 0.3985419198055893,
          "specificity": 0.916144675692059,
          "npv": 0.9826614651061986,
          "threshold": 0.09598103910684586,
          "positive_samples": 244,
          "true_positives": 164,
          "false_positives": 415,
          "true_negatives": 4534,
          "false_negatives": 80
        },
        "obscene": {
          "auc": 0.9387485218400053,
          "precision": 0.652962515114873,
          "recall": 0.8723747980613893,
          "f1": 0.7468879668049793,
          "specificity": 0.8548672566371681,
          "npv": 0.9553546199491382,
          "threshold": 0.1990610957145691,
          "positive_samples": 1238,
          "true_positives": 1080,
          "false_positives": 574,
          "true_negatives": 3381,
          "false_negatives": 158
        },
        "threat": {
          "auc": 0.9301419452470411,
          "precision": 0.3392070484581498,
          "recall": 0.7264150943396226,
          "f1": 0.4624624624624625,
          "specificity": 0.9705130725378416,
          "npv": 0.9941602899718083,
          "threshold": 0.07516312599182129,
          "positive_samples": 106,
          "true_positives": 77,
          "false_positives": 150,
          "true_negatives": 4937,
          "false_negatives": 29
        },
        "insult": {
          "auc": 0.911656762836887,
          "precision": 0.6324017053529133,
          "recall": 0.8794466403162056,
          "f1": 0.7357398732433177,
          "specificity": 0.7888435374149659,
          "npv": 0.9406229720960415,
          "threshold": 0.24214455485343933,
          "positive_samples": 1518,
          "true_positives": 1335,
          "false_positives": 776,
          "true_negatives": 2899,
          "false_negatives": 183
        },
        "identity_hate": {
          "auc": 0.9000925697269553,
          "precision": 0.39092872570194387,
          "recall": 0.6418439716312057,
          "f1": 0.48590604026845635,
          "specificity": 0.9425778863775198,
          "npv": 0.9786469344608879,
          "threshold": 0.15382634103298187,
          "positive_samples": 282,
          "true_positives": 181,
          "false_positives": 282,
          "true_negatives": 4629,
          "false_negatives": 101
        }
      }
    },
    "2": {
      "auc": 0.9038283798815403,
      "auc_ci": [
        0.8935430182079808,
        0.9144009635468395
      ],
      "precision": 0.522011473859128,
      "recall": 0.7274366059742697,
      "f1": 0.591615973249311,
      "f1_ci": [
        0.5741278187005213,
        0.6120481953342088
      ],
      "sample_count": 5163,
      "class_metrics": {
        "toxic": {
          "auc": 0.962186696069828,
          "precision": 0.8472752516487331,
          "recall": 0.9446594427244582,
          "f1": 0.8933211344922233,
          "specificity": 0.8293912369135323,
          "npv": 0.9373356704645048,
          "threshold": 0.23572076857089996,
          "positive_samples": 2584,
          "true_positives": 2441,
          "false_positives": 440,
          "true_negatives": 2139,
          "false_negatives": 143
        },
        "severe_toxic": {
          "auc": 0.8905198644266628,
          "precision": 0.24005681818181818,
          "recall": 0.6926229508196722,
          "f1": 0.35654008438818563,
          "specificity": 0.891238056515552,
          "npv": 0.9831800852209015,
          "threshold": 0.07072608172893524,
          "positive_samples": 244,
          "true_positives": 169,
          "false_positives": 535,
          "true_negatives": 4384,
          "false_negatives": 75
        },
        "obscene": {
          "auc": 0.9233059279915211,
          "precision": 0.5977464788732394,
          "recall": 0.8570274636510501,
          "f1": 0.7042814470627282,
          "specificity": 0.8180891719745222,
          "npv": 0.9477567886658795,
          "threshold": 0.17584121227264404,
          "positive_samples": 1238,
          "true_positives": 1061,
          "false_positives": 714,
          "true_negatives": 3211,
          "false_negatives": 177
        },
        "threat": {
          "auc": 0.8485785983807744,
          "precision": 0.35658914728682173,
          "recall": 0.42592592592592593,
          "f1": 0.3881856540084388,
          "specificity": 0.9835806132542038,
          "npv": 0.9876837504966229,
          "threshold": 0.17191633582115173,
          "positive_samples": 108,
          "true_positives": 46,
          "false_positives": 83,
          "true_negatives": 4972,
          "false_negatives": 62
        },
        "insult": {
          "auc": 0.8943137096607782,
          "precision": 0.55249500998004,
          "recall": 0.9141347424042272,
          "f1": 0.6887285394376711,
          "specificity": 0.6927925459029871,
          "npv": 0.9510910458991723,
          "threshold": 0.1587354838848114,
          "positive_samples": 1514,
          "true_positives": 1384,
          "false_positives": 1121,
          "true_negatives": 2528,
          "false_negatives": 130
        },
        "identity_hate": {
          "auc": 0.9040654827596764,
          "precision": 0.5379061371841155,
          "recall": 0.5302491103202847,
          "f1": 0.5340501792114696,
          "specificity": 0.9737812371978697,
          "npv": 0.9729840360212854,
          "threshold": 0.27445438504219055,
          "positive_samples": 281,
          "true_positives": 149,
          "false_positives": 128,
          "true_negatives": 4754,
          "false_negatives": 132
        }
      }
    },
    "3": {
      "auc": 0.92761210984972,
      "auc_ci": [
        0.9183436817715036,
        0.9359101553466326
      ],
      "precision": 0.5533412688686751,
      "recall": 0.7768262352491201,
      "f1": 0.6213292670121352,
      "f1_ci": [
        0.6144914071138835,
        0.6498367767700229
      ],
      "sample_count": 5168,
      "class_metrics": {
        "toxic": {
          "auc": 0.9747483574660654,
          "precision": 0.8939117754283631,
          "recall": 0.9507561070182241,
          "f1": 0.9214580984592259,
          "specificity": 0.8876013904982619,
          "npv": 0.9476288659793815,
          "threshold": 0.3153749406337738,
          "positive_samples": 2579,
          "true_positives": 2452,
          "false_positives": 291,
          "true_negatives": 2298,
          "false_negatives": 127
        },
        "severe_toxic": {
          "auc": 0.9073687265747946,
          "precision": 0.24854651162790697,
          "recall": 0.7066115702479339,
          "f1": 0.36774193548387096,
          "specificity": 0.8950466910272026,
          "npv": 0.9841517857142857,
          "threshold": 0.09330321848392487,
          "positive_samples": 242,
          "true_positives": 171,
          "false_positives": 517,
          "true_negatives": 4409,
          "false_negatives": 71
        },
        "obscene": {
          "auc": 0.9429228614622656,
          "precision": 0.698,
          "recall": 0.8491484184914841,
          "f1": 0.7661909989023051,
          "specificity": 0.884879288437103,
          "npv": 0.9492911668484187,
          "threshold": 0.28403133153915405,
          "positive_samples": 1233,
          "true_positives": 1047,
          "false_positives": 453,
          "true_negatives": 3482,
          "false_negatives": 186
        },
        "threat": {
          "auc": 0.8985232762406675,
          "precision": 0.34146341463414637,
          "recall": 0.6481481481481481,
          "f1": 0.4472843450479233,
          "specificity": 0.9733201581027668,
          "npv": 0.9923433407213379,
          "threshold": 0.07555428892374039,
          "positive_samples": 108,
          "true_positives": 70,
          "false_positives": 135,
          "true_negatives": 4925,
          "false_negatives": 38
        },
        "insult": {
          "auc": 0.9178884966596523,
          "precision": 0.6494895478852698,
          "recall": 0.8853545394300861,
          "f1": 0.749298934380258,
          "specificity": 0.8029516261273572,
          "npv": 0.9443908711025394,
          "threshold": 0.2742159962654114,
          "positive_samples": 1509,
          "true_positives": 1336,
          "false_positives": 721,
          "true_negatives": 2938,
          "false_negatives": 173
        },
        "identity_hate": {
          "auc": 0.9242209406948738,
          "precision": 0.48863636363636365,
          "recall": 0.6209386281588448,
          "f1": 0.5468998410174881,
          "specificity": 0.9631977100797383,
          "npv": 0.9781976744186046,
          "threshold": 0.19782738387584686,
          "positive_samples": 277,
          "true_positives": 172,
          "false_positives": 180,
          "true_negatives": 4711,
          "false_negatives": 105
        }
      }
    },
    "4": {
      "auc": 0.9194015203913777,
      "auc_ci": [
        0.9101500187954271,
        0.9283084930399836
      ],
      "precision": 0.544884606846566,
      "recall": 0.7619754807215592,
      "f1": 0.6228675316386015,
      "f1_ci": [
        0.6001781970051998,
        0.6355042063192523
      ],
      "sample_count": 5158,
      "class_metrics": {
        "toxic": {
          "auc": 0.9718317503718534,
          "precision": 0.900374531835206,
          "recall": 0.9357726741922927,
          "f1": 0.9177323916778011,
          "specificity": 0.8972576284279644,
          "npv": 0.9336816720257235,
          "threshold": 0.3578433394432068,
          "positive_samples": 2569,
          "true_positives": 2404,
          "false_positives": 266,
          "true_negatives": 2323,
          "false_negatives": 165
        },
        "severe_toxic": {
          "auc": 0.8962662667751119,
          "precision": 0.22033898305084745,
          "recall": 0.7583333333333333,
          "f1": 0.34146341463414637,
          "specificity": 0.8690524603497357,
          "npv": 0.9866112650046168,
          "threshold": 0.06426077336072922,
          "positive_samples": 240,
          "true_positives": 182,
          "false_positives": 644,
          "true_negatives": 4274,
          "false_negatives": 58
        },
        "obscene": {
          "auc": 0.940124596695143,
          "precision": 0.6627475247524752,
          "recall": 0.875,
          "f1": 0.754225352112676,
          "specificity": 0.8614641586171835,
          "npv": 0.9568040654997176,
          "threshold": 0.20974239706993103,
          "positive_samples": 1224,
          "true_positives": 1071,
          "false_positives": 545,
          "true_negatives": 3389,
          "false_negatives": 153
        },
        "threat": {
          "auc": 0.8861722579224618,
          "precision": 0.32967032967032966,
          "recall": 0.5607476635514018,
          "f1": 0.41522491349480967,
          "specificity": 0.9758463670560286,
          "npv": 0.9905546623794212,
          "threshold": 0.10153984278440475,
          "positive_samples": 107,
          "true_positives": 60,
          "false_positives": 122,
          "true_negatives": 4929,
          "false_negatives": 47
        },
        "insult": {
          "auc": 0.9083470996902684,
          "precision": 0.6083821541234791,
          "recall": 0.8982035928143712,
          "f1": 0.7254164427727029,
          "specificity": 0.7622435020519835,
          "npv": 0.9479414766927526,
          "threshold": 0.20908455550670624,
          "positive_samples": 1503,
          "true_positives": 1350,
          "false_positives": 869,
          "true_negatives": 2786,
          "false_negatives": 153
        },
        "identity_hate": {
          "auc": 0.9136671508934282,
          "precision": 0.5477941176470589,
          "recall": 0.5437956204379562,
          "f1": 0.5457875457875457,
          "specificity": 0.9748157248157249,
          "npv": 0.9744167007777323,
          "threshold": 0.29832160472869873,
          "positive_samples": 274,
          "true_positives": 149,
          "false_positives": 123,
          "true_negatives": 4761,
          "false_negatives": 125
        }
      }
    },
    "5": {
      "auc": 0.9219750736133673,
      "auc_ci": [
        0.9119932953035416,
        0.9321281229517155
      ],
      "precision": 0.5229086357645957,
      "recall": 0.8063978951953917,
      "f1": 0.622170060479389,
      "f1_ci": [
        0.5971069353335262,
        0.6287643393654693
      ],
      "sample_count": 5146,
      "class_metrics": {
        "toxic": {
          "auc": 0.9757415342563087,
          "precision": 0.9041353383458647,
          "recall": 0.9350699844479005,
          "f1": 0.9193425076452599,
          "specificity": 0.9009324009324009,
          "npv": 0.9328238133547868,
          "threshold": 0.3548751771450043,
          "positive_samples": 2572,
          "true_positives": 2405,
          "false_positives": 255,
          "true_negatives": 2319,
          "false_negatives": 167
        },
        "severe_toxic": {
          "auc": 0.9032281899714635,
          "precision": 0.23772609819121446,
          "recall": 0.7666666666666667,
          "f1": 0.3629191321499014,
          "specificity": 0.8797390949857318,
          "npv": 0.9871912168344007,
          "threshold": 0.0854898989200592,
          "positive_samples": 240,
          "true_positives": 184,
          "false_positives": 590,
          "true_negatives": 4316,
          "false_negatives": 56
        },
        "obscene": {
          "auc": 0.9399297347094919,
          "precision": 0.6948480845442536,
          "recall": 0.8587755102040816,
          "f1": 0.7681635633442863,
          "specificity": 0.8821729150726856,
          "npv": 0.9523678414096917,
          "threshold": 0.2372274100780487,
          "positive_samples": 1225,
          "true_positives": 1052,
          "false_positives": 462,
          "true_negatives": 3459,
          "false_negatives": 173
        },
        "threat": {
          "auc": 0.8786647405643039,
          "precision": 0.2624113475177305,
          "recall": 0.6851851851851852,
          "f1": 0.37948717948717947,
          "specificity": 0.9587137753076618,
          "npv": 0.9930098684210527,
          "threshold": 0.05637403577566147,
          "positive_samples": 108,
          "true_positives": 74,
          "false_positives": 208,
          "true_negatives": 4830,
          "false_negatives": 34
        },
        "insult": {
          "auc": 0.9170891169219626,
          "precision": 0.6771694214876033,
          "recall": 0.8716755319148937,
          "f1": 0.7622093023255814,
          "specificity": 0.828390993959363,
          "npv": 0.9398753894080997,
          "threshold": 0.32249945402145386,
          "positive_samples": 1504,
          "true_positives": 1311,
          "false_positives": 625,
          "true_negatives": 3017,
          "false_negatives": 193
        },
        "identity_hate": {
          "auc": 0.9171971252566736,
          "precision": 0.36116152450090744,
          "recall": 0.7210144927536232,
          "f1": 0.48125755743651755,
          "specificity": 0.9277207392197125,
          "npv": 0.9832426550598476,
          "threshold": 0.12141162902116776,
          "positive_samples": 276,
          "true_positives": 199,
          "false_positives": 352,
          "true_negatives": 4518,
          "false_negatives": 77
        }
      }
    },
    "6": {
      "auc": 0.9243019505301779,
      "auc_ci": [
        0.9156519845572569,
        0.9331524982911324
      ],
      "precision": 0.5533406856075316,
      "recall": 0.7782040490120726,
      "f1": 0.6308475345119618,
      "f1_ci": [
        0.6123481918967906,
        0.6474149332033056
      ],
      "sample_count": 5192,
      "class_metrics": {
        "toxic": {
          "auc": 0.9780732995232398,
          "precision": 0.8990120746432492,
          "recall": 0.9538043478260869,
          "f1": 0.925598041062347,
          "specificity": 0.8944954128440367,
          "npv": 0.9516063440422936,
          "threshold": 0.32322922348976135,
          "positive_samples": 2576,
          "true_positives": 2457,
          "false_positives": 276,
          "true_negatives": 2340,
          "false_negatives": 119
        },
        "severe_toxic": {
          "auc": 0.906757659236998,
          "precision": 0.25337331334332835,
          "recall": 0.6983471074380165,
          "f1": 0.3718371837183718,
          "specificity": 0.8993939393939394,
          "npv": 0.9838674033149172,
          "threshold": 0.08826054632663727,
          "positive_samples": 242,
          "true_positives": 169,
          "false_positives": 498,
          "true_negatives": 4452,
          "false_negatives": 73
        },
        "obscene": {
          "auc": 0.9375048626461149,
          "precision": 0.6970926301555105,
          "recall": 0.8354943273905997,
          "f1": 0.7600442314780685,
          "specificity": 0.886811520970187,
          "npv": 0.9453272286560732,
          "threshold": 0.27451014518737793,
          "positive_samples": 1234,
          "true_positives": 1031,
          "false_positives": 448,
          "true_negatives": 3510,
          "false_negatives": 203
        },
        "threat": {
          "auc": 0.9031869137455848,
          "precision": 0.29770992366412213,
          "recall": 0.7155963302752294,
          "f1": 0.42048517520215634,
          "specificity": 0.9638009049773756,
          "npv": 0.993711967545639,
          "threshold": 0.054793357849121094,
          "positive_samples": 109,
          "true_positives": 78,
          "false_positives": 184,
          "true_negatives": 4899,
          "false_negatives": 31
        },
        "insult": {
          "auc": 0.916483807029724,
          "precision": 0.6481036965914546,
          "recall": 0.8940397350993378,
          "f1": 0.7514611745059838,
          "specificity": 0.8009234111895709,
          "npv": 0.9485365069154069,
          "threshold": 0.2600024938583374,
          "positive_samples": 1510,
          "true_positives": 1350,
          "false_positives": 733,
          "true_negatives": 2949,
          "false_negatives": 160
        },
        "identity_hate": {
          "auc": 0.9038051609994056,
          "precision": 0.5247524752475248,
          "recall": 0.5719424460431655,
          "f1": 0.5473321858864028,
          "specificity": 0.9706959706959707,
          "npv": 0.9756596440989977,
          "threshold": 0.25063619017601013,
          "positive_samples": 278,
          "true_positives": 159,
          "false_positives": 144,
          "true_negatives": 4770,
          "false_negatives": 119
        }
      }
    }
  },
  "per_class": {},
  "thresholds": {
    "0": {
      "toxic": 0.2747279405593872,
      "severe_toxic": 0.08588284254074097,
      "obscene": 0.37424641847610474,
      "threat": 0.11552593857049942,
      "insult": 0.26808470487594604,
      "identity_hate": 0.1609325259923935
    },
    "1": {
      "toxic": 0.2784601151943207,
      "severe_toxic": 0.09598103910684586,
      "obscene": 0.1990610957145691,
      "threat": 0.07516312599182129,
      "insult": 0.24214455485343933,
      "identity_hate": 0.15382634103298187
    },
    "2": {
      "toxic": 0.23572076857089996,
      "severe_toxic": 0.07072608172893524,
      "obscene": 0.17584121227264404,
      "threat": 0.17191633582115173,
      "insult": 0.1587354838848114,
      "identity_hate": 0.27445438504219055
    },
    "3": {
      "toxic": 0.3153749406337738,
      "severe_toxic": 0.09330321848392487,
      "obscene": 0.28403133153915405,
      "threat": 0.07555428892374039,
      "insult": 0.2742159962654114,
      "identity_hate": 0.19782738387584686
    },
    "4": {
      "toxic": 0.3578433394432068,
      "severe_toxic": 0.06426077336072922,
      "obscene": 0.20974239706993103,
      "threat": 0.10153984278440475,
      "insult": 0.20908455550670624,
      "identity_hate": 0.29832160472869873
    },
    "5": {
      "toxic": 0.3548751771450043,
      "severe_toxic": 0.0854898989200592,
      "obscene": 0.2372274100780487,
      "threat": 0.05637403577566147,
      "insult": 0.32249945402145386,
      "identity_hate": 0.12141162902116776
    },
    "6": {
      "toxic": 0.32322922348976135,
      "severe_toxic": 0.08826054632663727,
      "obscene": 0.27451014518737793,
      "threat": 0.054793357849121094,
      "insult": 0.2600024938583374,
      "identity_hate": 0.25063619017601013
    }
  }
}